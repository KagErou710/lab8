{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = '''\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input}\n",
    "\n",
    "### Response:\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "# Set GPU device\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "# os.environ['http_proxy']  = 'http://192.41.170.23:3128'\n",
    "# os.environ['https_proxy'] = 'http://192.41.170.23:3128'\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the dataset\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer\n",
    "import datasets\n",
    "print(datasets.__version__)\n",
    "\n",
    "# dataset = load_dataset(\"imdb\", split=\"train\")\n",
    "\n",
    "# dataset\n",
    "\n",
    "# file_path = r\"C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab8\\alpaca_data.json\"\n",
    "file_path = r\"C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab8\\alpaca_eval.json\"\n",
    "dataset = load_dataset(\"json\", data_files=file_path, split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['### Question: What are the names of some famous actors that started their careers on Broadway?\\n ### Answer: Some famous actors that started their careers on Broadway include: \\n1. Hugh Jackman \\n2. Meryl Streep \\n3. Denzel Washington \\n4. Julia Roberts \\n5. Christopher Walken \\n6. Anthony Rapp \\n7. Audra McDonald \\n8. Nathan Lane \\n9. Sarah Jessica Parker \\n10. Lin-Manuel Miranda',\n",
       " '### Question: How did US states get their names?\\n ### Answer: US states get their names from a variety of sources, including Native American tribes, Spanish explorers, British colonists, and even presidents. For example, the state of Alabama was named after the Native American tribe that lived in the area, while the state of Florida gets its name from the Spanish explorer, Ponce de Leon, who explored the area in the 1500s. Other states are named after English kings (like Virginia, named after England\\'s \"Virgin Queen,\" Queen Elizabeth I) or presidents (like Washington, named after George Washington).']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['instruction'])):\n",
    "        text = f\"### Question: {example['instruction'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "\n",
    "    # output_texts = []\n",
    "    # for i in range(len(example['instruction'])):\n",
    "    #     text =f\"\"\"\n",
    "    #     Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    #     ### Instruction:\n",
    "    #     {example['instruction'][i]}\n",
    "\n",
    "    #     ### Answer:\n",
    "    #     {example['output'][i]}\n",
    "    #     \"\"\".strip()\n",
    "    # output_texts.append(text)\n",
    "    # return output_texts\n",
    "    \n",
    "\n",
    "#check instruction-prompt\n",
    "formatting_prompts_func(dataset[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Tairo Kageyama\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\trl\\trainer\\sft_trainer.py:245: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "  0%|          | 0/114 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 114/114 [32:33<00:00, 17.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1953.516, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.058, 'train_loss': 2.21263965807463, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=114, training_loss=2.21263965807463, metrics={'train_runtime': 1953.516, 'train_samples_per_second': 0.461, 'train_steps_per_second': 0.058, 'train_loss': 2.21263965807463, 'epoch': 3.0})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_template = \" ### Answer:\"\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template, tokenizer=tokenizer)\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset=dataset.select(range(300)),\n",
    "    formatting_func=formatting_prompts_func,\n",
    "    data_collator=collator,\n",
    ")\n",
    "\n",
    "trainer.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(r\"C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab8\\models\\V2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilgpt2\", device_map = 'auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilgpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "\n",
      "\n",
      "\n",
      "The song \"I'm a Girl\" is a song about a girl who is forced to accept the fact that she is a girl. She is forced into a relationship with a man who is a man and a woman. She has to accept that she has to be a man, and that she must be a woman to be able to be loved. She must be able and able to have a happy life. She needs to be happy, and she must have a good life.\n",
      "The lyrics of the song \"My Love Is My Love\" are a perfect example of how the\n"
     ]
    }
   ],
   "source": [
    "# alpaca_data\n",
    "\n",
    "\n",
    "model2 = AutoModelForCausalLM.from_pretrained(r\"C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab8\\models\\V1\",\n",
    "                                              load_in_8bit = False)\n",
    "model3 = AutoModelForCausalLM.from_pretrained(r\"C:\\Users\\Tairo Kageyama\\Documents\\GitHub\\Python-fo-Natural-Language-Processing-main\\lab8\\models\\V2\",\n",
    "                                              load_in_8bit = False)\n",
    "# prompt = \"How many songs have been recorded throughout history? Try to explain your answer. Your explanation should take the reader through your reasoning step-by-step.\"\n",
    "prompt = \"How many songs have been recorded throughout history?\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "max_length = 128\n",
    "no_repeat_ngram_size = 3\n",
    "\n",
    "outputs = model2.generate(input_ids=input_ids, max_length=max_length, no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "print(len(outputs[0]))\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.replace(prompt, \"\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "\n",
      "\n",
      "\n",
      "The first song, “The Last Time”, was recorded in 1887. It was recorded on a small island in the southern part of the island of Bering Sea. It is said to have been written by a man named John Bering, who was a member of the British Parliament. It has been recorded in the same way that the first song was recorded. It also has a number of other songs recorded throughout the history of the United Kingdom.\n",
      "\n",
      "In 1887, the British government announced that it would be opening a new chapter in the British history of\n"
     ]
    }
   ],
   "source": [
    "# alpaca eval\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "max_length = 128\n",
    "no_repeat_ngram_size = 3\n",
    "\n",
    "outputs = model3.generate(input_ids=input_ids, max_length=max_length, no_repeat_ngram_size=no_repeat_ngram_size)\n",
    "print(len(outputs[0]))\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "generated_text = generated_text.replace(prompt, \"\")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
